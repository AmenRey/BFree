{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCZBFzjClURz"
   },
   "source": [
    "# Train a Simple model for running on the BFree\n",
    "\n",
    "(This code is based on [Tensorflow Lite example](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb))\n",
    "\n",
    "This notebook demonstrates the process of training a 4 kB model using TensorFlow and converting it for use with TensorFlow Lite for usage with the BFree.\n",
    "\n",
    "Deep learning networks learn to model patterns in underlying data. Here, we're going to train a network to model data generated by a [sine](https://en.wikipedia.org/wiki/Sine) function. This will result in a model that can take a value, `x`, and predict its sine, `y`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UQblnrLd_ET"
   },
   "source": [
    "## Configure Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mGqmpSE5jt0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Ensure models directory exists\n",
    "model_dir = \"neural_net/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_extension = \"net\"\n",
    "\n",
    "# Define layers for each model\n",
    "model_names = [\"sine-16-16\", \"sine-16-16-16\", \"sine-32-32\", \"sine-64-32\", \"sine-64-64\"]\n",
    "models = {\n",
    "    model_names[0]: [{\"units\": 16, \"activation\": 'relu', \"input_shape\": (1,)},\n",
    "                   {\"units\": 16, \"activation\": 'relu'}],\n",
    "    model_names[1]: [{\"units\": 16, \"activation\": 'relu', \"input_shape\": (1,)},\n",
    "                   {\"units\": 16, \"activation\": 'relu'},\n",
    "                   {\"units\": 16, \"activation\": 'relu'}],\n",
    "    model_names[2]: [{\"units\": 32, \"activation\": 'relu', \"input_shape\": (1,)},\n",
    "                   {\"units\": 32, \"activation\": 'relu'}],\n",
    "    model_names[3]: [{\"units\": 64, \"activation\": 'relu', \"input_shape\": (1,)},\n",
    "                   {\"units\": 32, \"activation\": 'relu'}],\n",
    "    model_names[4]: [{\"units\": 64, \"activation\": 'relu', \"input_shape\": (1,)},\n",
    "                   {\"units\": 64, \"activation\": 'relu'}],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rxPJMPU_ci2"
   },
   "outputs": [],
   "source": [
    "def export_model(md):\n",
    "  \"\"\"Export a keras model to an nngb string representation\"\"\"\n",
    "  result = \"\"\n",
    "  result += str(len(md.layers)) + \"\\n\"\n",
    "  for l in md.layers:\n",
    "    insize = l.input.shape[1]\n",
    "    outsize = l.output.shape[1]\n",
    "    result += \"Layer(\" + str(insize) + \", \" + str(outsize)\n",
    "    if (l.activation == keras.activations.relu):\n",
    "      result += \", activator = lambda x:max(0, x)\"\n",
    "    result += \")\" + \"\\n\"\n",
    "    for n in l.weights[0].numpy().flat:\n",
    "      result += str(n) + \"\\n\"\n",
    "    for n in l.weights[1].numpy().flat:\n",
    "      result += str(n) + \"\\n\"\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dh4AXGuHWeu1"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cr1VLfotanf6"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx9lOPWh9grN"
   },
   "source": [
    "Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53PBJBv1jEtJ"
   },
   "outputs": [],
   "source": [
    "# TensorFlow is an open source machine learning library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras is TensorFlow's high-level API for deep learning\n",
    "from tensorflow import keras\n",
    "# Numpy is a math library\n",
    "import numpy as np\n",
    "# Pandas is a data manipulation library \n",
    "import pandas as pd\n",
    "# Matplotlib is a graphing library\n",
    "import matplotlib.pyplot as plt\n",
    "# Math is Python's math library\n",
    "import math\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-PuBEb6CMeo"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gB0-dlNmLT-"
   },
   "source": [
    "### 1. Generate Data\n",
    "\n",
    "The code in the following cell will generate a set of random `x` values, calculate their sine values, and display them on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKjg7QeMDsDx"
   },
   "outputs": [],
   "source": [
    "# Number of sample datapoints\n",
    "SAMPLES = 2000\n",
    "\n",
    "# Generate a uniformly distributed set of random numbers in the range from\n",
    "# 0 to 2Ï€, which covers a complete sine wave oscillation\n",
    "x_values = np.random.uniform(\n",
    "    low=0, high=2*math.pi, size=SAMPLES).astype(np.float32)\n",
    "\n",
    "# Shuffle the values to guarantee they're not in order\n",
    "np.random.shuffle(x_values)\n",
    "\n",
    "# Calculate the corresponding sine values\n",
    "y_values = np.sin(x_values).astype(np.float32)\n",
    "\n",
    "# Plot our data. The 'b.' argument tells the library to print blue dots.\n",
    "plt.plot(x_values, y_values, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWOlC7W_FYvA"
   },
   "source": [
    "### 2. Add Noise\n",
    "Since it was generated directly by the sine function, our data fits a nice, smooth curve.\n",
    "\n",
    "However, machine learning models are good at extracting underlying meaning from messy, real world data. To demonstrate this, we can add some noise to our data to approximate something more life-like.\n",
    "\n",
    "In the following cell, we'll add some random noise to each value, then draw a new graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0FJe3Y-Gkac"
   },
   "outputs": [],
   "source": [
    "# Add a small random number to each y value\n",
    "x_values += 0.1 * np.random.randn(*x_values.shape)\n",
    "\n",
    "# Plot our data\n",
    "plt.plot(x_values, y_values, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTPSB1Gt7AW2"
   },
   "outputs": [],
   "source": [
    "# Trim values outside the valid range\n",
    "valid_x = [x_values[i] > 0 and x_values[i] < 2*math.pi for i in range(len(x_values))]\n",
    "x_values = x_values[valid_x]\n",
    "y_values = y_values[valid_x]\n",
    "\n",
    "# Plot our trimmed data\n",
    "plt.plot(x_values, y_values, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up8Xk_pMH4Rt"
   },
   "source": [
    "### 3. Split the Data\n",
    "We now have a noisy dataset that approximates real world data. We'll be using this to train our model.\n",
    "\n",
    "To evaluate the accuracy of the model we train, we'll need to compare its predictions to real data and check how well they match up. This evaluation happens during training (where it is referred to as validation) and after training (referred to as testing) It's important in both cases that we use fresh data that was not already used to train the model.\n",
    "\n",
    "The data is split as follows:\n",
    "  1. Training: 60%\n",
    "  2. Validation: 20%\n",
    "  3. Testing: 20% \n",
    "\n",
    "The following code will split our data and then plots each set as a different color:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNYko5L1keqZ"
   },
   "outputs": [],
   "source": [
    "# We'll use 60% of our data for training and 20% for testing. The remaining 20%\n",
    "# will be used for validation. Calculate the indices of each section.\n",
    "TRAIN_SPLIT =  int(0.6 * len(x_values))\n",
    "TEST_SPLIT = int(0.2 * len(x_values) + TRAIN_SPLIT)\n",
    "\n",
    "# Use np.split to chop our data into three parts.\n",
    "# The second argument to np.split is an array of indices where the data will be\n",
    "# split. We provide two indices, so the data will be divided into three chunks.\n",
    "x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "\n",
    "# Plot the data in each partition in different colors:\n",
    "plt.plot(x_train, y_train, 'b.', label=\"Train\")\n",
    "plt.plot(x_test, y_test, 'r.', label=\"Test\")\n",
    "plt.plot(x_validate, y_validate, 'y.', label=\"Validate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7sL-hWtoAZC"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmEcS96e8-vR"
   },
   "source": [
    "## 1. Build the model\n",
    "\n",
    "First we choose a model, then we prepare it with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW0xus6AF-4o"
   },
   "outputs": [],
   "source": [
    "model_name = model_names[4]\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Generate layers using model parameter data\n",
    "for layer in models[model_name]:\n",
    "  model.add(keras.layers.Dense(**layer))\n",
    "\n",
    "# Final layer is a single neuron, since we want to output a single value\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "# Compile the model using the standard 'adam' optimizer and the mean squared error or 'mse' loss function for regression.\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv2SC409Grap"
   },
   "source": [
    "### 2. Train the Model ###\n",
    "\n",
    "We'll now train the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPAUrdkmGq1M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=64,\n",
    "                    validation_data=(x_validate, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc_CQu2_IvOP"
   },
   "source": [
    "### 3. Plot Metrics\n",
    "Each training epoch, the model prints out its loss and mean absolute error for training and validation. You can read this in the output above (note that your exact numbers may differ): \n",
    "\n",
    "```\n",
    "Epoch 200/200\n",
    "19/19 [==============================] - 0s 3ms/step - loss: 0.0052 - mae: 0.0556 - val_loss: 0.0065 - val_mae: 0.0631\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYHGswAJJgrC"
   },
   "outputs": [],
   "source": [
    "# Draw a graph of the loss, which is the distance between\n",
    "# the predicted and actual values during training and validation.\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Exclude the first few epochs so the graph is easier to read\n",
    "SKIP = 100\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(epochs[SKIP:], train_loss[SKIP:], 'g.', label='Training loss')\n",
    "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Draw a graph of mean absolute error, which is another way of\n",
    "# measuring the amount of error in the prediction.\n",
    "train_mae = history.history['mae']\n",
    "val_mae = history.history['val_mae']\n",
    "\n",
    "plt.plot(epochs[SKIP:], train_mae[SKIP:], 'g.', label='Training MAE')\n",
    "plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\n",
    "plt.title('Training and validation mean absolute error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f86dWOyZKmN9"
   },
   "source": [
    "Great results! From these graphs, we can see several exciting things:\n",
    "\n",
    "*   The overall loss and MAE are much better than our previous network\n",
    "*   Metrics are better for validation than training, which means the network is not overfitting\n",
    "\n",
    "The reason the metrics for validation are better than those for training is that validation metrics are calculated at the end of each epoch, while training metrics are calculated throughout the epoch, so validation happens on a model that has been trained slightly longer.\n",
    "\n",
    "This all means our network seems to be performing well! To confirm, let's check its predictions against the test dataset we set aside earlier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZfztKKyhLxX"
   },
   "outputs": [],
   "source": [
    "# Calculate and print the loss on our test dataset\n",
    "test_loss, test_mae = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Make predictions based on our test dataset\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Graph the predictions against the actual values\n",
    "plt.clf()\n",
    "plt.title('Comparison of predictions and actual values')\n",
    "plt.plot(x_test, y_test, 'b.', label='Actual values')\n",
    "plt.plot(x_test, y_test_pred, 'r.', label='TF predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fR0-ANz9M94"
   },
   "source": [
    "## 4. Save the model\n",
    "\n",
    "If it looks good, we can now save the model to a nngb string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Of2nuyi5IYy"
   },
   "outputs": [],
   "source": [
    "with open(f\"{model_dir}/{model_name}.{model_extension}\", 'w') as fh:\n",
    "    fh.write(export_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the model and library\n",
    "\n",
    "Import the library, import the exported model, and run it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neural net library\n",
    "from neural_net_src.net import Net\n",
    "\n",
    "# Generate predictions\n",
    "y_test_pred = []\n",
    "net = Net(f\"{model_dir}/{model_name}.{model_extension}\")\n",
    "for x in x_test:\n",
    "    y_test_pred.append(net.infer([x]))\n",
    "\n",
    "# Graph the predictions against the actual values\n",
    "plt.clf()\n",
    "plt.title('Comparison of predictions and actual values')\n",
    "plt.plot(x_test, y_test, 'b.', label='Actual values')\n",
    "plt.plot(x_test, y_test_pred, 'r.', label='TF predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of train_hello_world_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
